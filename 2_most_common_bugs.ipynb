{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shahabday/DSR-debugging-dl-models/blob/master/2_most_common_bugs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Please, make a copy of the notebook.\n",
        "import gdown\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "\n",
        "from sklearn.datasets import make_regression\n",
        "\n",
        "%matplotlib inline\n",
        "%load_ext tensorboard"
      ],
      "metadata": {
        "id": "WlmLVphsddpQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Load tensorboard logs and create some helper functions."
      ],
      "metadata": {
        "id": "OGk9EIPHWbCz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = f'https://drive.google.com/uc?id=1baY_ylqS9_kk-LIuZIU8xMLXRW6_saTh'\n",
        "\n",
        "output = 'tb_logs.zip'\n",
        "gdown.download(url, output, quiet=False)\n",
        "\n",
        "# unzip\n",
        "!unzip tb_logs.zip\n",
        "!rm tb_logs.zip"
      ],
      "metadata": {
        "id": "RwfQie_k17Mt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_writer(filepath, dir_name):\n",
        "    \"\"\" Creates a directory to save tensorboard events \"\"\"\n",
        "    path = os.path.join(filepath, dir_name)\n",
        "    os.makedirs(path, exist_ok=True)\n",
        "    print(f'Creating a tensorboard directory: {path}')\n",
        "    writer = tf.summary.create_file_writer(path)\n",
        "    return writer\n",
        "\n",
        "def standardize(array):\n",
        "    means = np.mean(array, axis=0)\n",
        "    std = np.std(array, axis=0)\n",
        "    return (array - means) /  std"
      ],
      "metadata": {
        "id": "1iJ26ohqPov0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8CIsnKUbPXro"
      },
      "source": [
        "# Most common bugs I\n",
        "\n",
        "## Resources\n",
        "\n",
        "- [Chapter 4 of Deep learning book. Numerical computation](https://www.deeplearningbook.org/contents/numerical.html)\n",
        "- [Gradient norm clipping](http://proceedings.mlr.press/v28/pascanu13.html)\n",
        "- [The log-sum-exp trick](https://gregorygundersen.com/blog/2020/02/09/log-sum-exp/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRNcy-4fPXrq"
      },
      "source": [
        "## Incorrect tensor shapes\n",
        "\n",
        "### Most common reasons:\n",
        "\n",
        "- Flipped dimensions when using tf.reshape.\n",
        "- Sum, avg, softmax over wrong dimension.\n",
        "- Forgot to flatten after conv layers.\n",
        "- Forgot to get rid of extra \"1\" dimensions, e.g. if shape is (None, 1, 1, 4)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qv9xfKs-PXrr"
      },
      "source": [
        "- In TF2, as well as in other libraries, you can accidentally broadcast tensors and then it can fail silently or just output wrong results."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_true = np.array([0.1, 0.7, 0.02, 0.08, 0.05, 0.05])\n",
        "y_true_extra_dim = np.expand_dims(y_true, -1)\n",
        "y_pred = np.array([0.1, 0.6, 0.05, 0.05, 0.1, 0.1])"
      ],
      "metadata": {
        "id": "qDcgFJLk808l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'y_true: {y_true} \\n')\n",
        "print(f'Shape of y_true: {y_true.shape} \\n')\n",
        "print(f'y_true_extra_dim: {y_true_extra_dim} \\n')\n",
        "print(f'Shape of y_true_extra_dim: {y_true_extra_dim.shape} \\n')"
      ],
      "metadata": {
        "id": "JKnF4yJv0r5Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred"
      ],
      "metadata": {
        "id": "uDgpoypY0tJN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred.shape"
      ],
      "metadata": {
        "id": "LqcD_yvF0uPj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0UCFrN5jPXrw"
      },
      "source": [
        "Say we want to divide y_true by y_pred. What shapes do we expect to get?"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_true / y_pred"
      ],
      "metadata": {
        "id": "DDe5Aet00vlu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_true_extra_dim / y_pred"
      ],
      "metadata": {
        "id": "376ZV4qF0wf4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLMjHrqKPXrz"
      },
      "source": [
        "#### KL-divergence\n",
        "\n",
        "KL-divergence is used in some models like VAEs or Bayesian models."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "kl = tf.keras.losses.KLDivergence()\n",
        "\n",
        "print(f'KLD for y_true: {kl(y_true, y_pred).numpy()} \\n')\n",
        "print(f'KLD for y_true_extra_dim: {kl(y_true_extra_dim, y_pred).numpy()}')"
      ],
      "metadata": {
        "id": "cAoAElp10yAC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDcQazzbPXr0"
      },
      "source": [
        "## Pre-processing inputs incorrectly\n",
        "\n",
        "- Forgot to standardize/scale.\n",
        "    -  It makes the resulting model dependent on the choice of units used in the input.\n",
        "- Too much augmentation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1HE0mv6WPXr0"
      },
      "source": [
        "\n",
        "### Regression example with Auto MPG data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrND6j-9PXr1"
      },
      "source": [
        "#### Load the data and create a pandas DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data'\n",
        "column_names = ['MPG','Cylinders','Displacement','Horsepower','Weight',\n",
        "                'Acceleration', 'Model Year', 'Origin']\n",
        "dataset = pd.read_csv(url, names=column_names,\n",
        "                      na_values = \"?\", comment='\\t',\n",
        "                      sep=\" \", skipinitialspace=True)\n",
        "dataset = dataset.dropna()\n",
        "dataset.drop('Origin', axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "NTTgkHXs9Z7F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5_dRrXJPXr4"
      },
      "source": [
        "#### Create labels and train set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z5jROJTmPXr4"
      },
      "outputs": [],
      "source": [
        "labels = dataset.pop('MPG')\n",
        "labels = np.array(labels).astype('float32')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hF0nRMS9PXr4"
      },
      "source": [
        "Let's make the difference in scales for some features even more pronounced and print the statistics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B-yV_vjAPXr5"
      },
      "outputs": [],
      "source": [
        "dataset['Horsepower'] = dataset['Horsepower'] * 1000\n",
        "dataset['Displacement'] = dataset['Displacement'] / 1000\n",
        "train_set = np.array(dataset).astype('float32')\n",
        "train_set_stand = standardize(train_set)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Train_set std:\")\n",
        "{col: avg for col, avg in zip(dataset.columns, train_set.std(axis=0))}"
      ],
      "metadata": {
        "id": "Y_xes5oM00oI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Train_set_stand std:\")\n",
        "{col: avg for col, avg in zip(dataset.columns, train_set_stand.mean(axis=0))}"
      ],
      "metadata": {
        "id": "KPan8-C8018i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QSpsslD9PXr5"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class RegressorNet(tf.keras.Model):\n",
        "\n",
        "    def __init__(self, input_shape, optimizer):\n",
        "        super(RegressorNet, self).__init__()\n",
        "\n",
        "        self.optimizer = optimizer\n",
        "        self.regressor = tf.keras.Sequential([\n",
        "            tf.keras.layers.Input((input_shape, )),\n",
        "            tf.keras.layers.Dense(64, activation='relu', name='dense_1'),\n",
        "            tf.keras.layers.Dense(64, activation='relu', name='dense_2'),\n",
        "            tf.keras.layers.Dense(1, activation='linear', name='dense_out')\n",
        "        ])\n",
        "\n",
        "        self.loss = tf.keras.losses.MeanSquaredError()\n",
        "\n",
        "    def summary(self):\n",
        "        self.regressor.summary()\n",
        "\n",
        "    def call(self, X):\n",
        "        return self.regressor(X)\n",
        "\n",
        "    def get_loss(self, X, y_true):\n",
        "        y_pred = self(X)\n",
        "        l2_loss = self.loss(y_true, y_pred)\n",
        "        return l2_loss\n",
        "\n",
        "    def grad_step(self, X, y_true):\n",
        "        with tf.GradientTape() as tape:\n",
        "            loss = self.get_loss(X, y_true)\n",
        "        gradients = tape.gradient(loss, self.trainable_variables)\n",
        "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
        "        return loss, gradients\n",
        "\n",
        "optimizer_not_scaled = tf.keras.optimizers.Adam()\n",
        "optimizer_scaled = tf.keras.optimizers.Adam()\n",
        "\n",
        "net_not_scaled = RegressorNet(input_shape=train_set.shape[1], optimizer=optimizer_not_scaled)\n",
        "net_scaled = RegressorNet(input_shape=train_set.shape[1], optimizer=optimizer_scaled)\n",
        "\n",
        "net_scaled.summary()"
      ],
      "metadata": {
        "id": "z8I5i1K803ft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6c0z38WPXr7"
      },
      "source": [
        "### Train and output to Tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5n1vorGePXr7"
      },
      "outputs": [],
      "source": [
        "def train(model, epochs, X, y, save_dir):\n",
        "\n",
        "    writer = make_writer(os.path.join('tb_logs'), save_dir)\n",
        "\n",
        "    for epoch in range(0, epochs + 1):\n",
        "\n",
        "        if epoch % 100 == 0:\n",
        "            print('Epoch {} is running...'.format(epoch))\n",
        "\n",
        "        # Gradient update step\n",
        "        loss, gradients = model.grad_step(X, y.reshape(-1, 1))\n",
        "\n",
        "        if epoch % 100 == 0:\n",
        "            print(f'{loss}')\n",
        "\n",
        "        # Tensorboard\n",
        "        with writer.as_default():\n",
        "            tf.summary.scalar('Train loss', loss, step=epoch)\n",
        "\n",
        "            for layer_number, layer in enumerate(model.trainable_variables):\n",
        "                tf.summary.histogram('/'.join(layer.path.split('/')[1:]), gradients[layer_number], step=epoch, buckets=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#train(net_not_scaled, 1000, train_set, labels, 'scaling/regression_non_standard')\n",
        "#train(net_scaled, 1000, train_set_stand, labels, 'scaling/regression_standard')"
      ],
      "metadata": {
        "id": "i0-V_lFQlpHQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir tb_logs/scaling --port 6006"
      ],
      "metadata": {
        "id": "pR_jyNHl06HU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucIT6kQ9PXr9"
      },
      "source": [
        "## Incorrect input to the loss/ incorrect loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uiUnAS7uPXr9"
      },
      "source": [
        "- Softmaxed outputs to a loss that expects logits or vice-versa.\n",
        "- One-hot encoded labels to a sparse categorical cross-entropy loss.\n",
        "- ReLU in the last layer for regression problems.\n",
        "- E.g. MSE loss when categorical loss is expected."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Log-sum-exp trick:\n",
        "\n",
        "Softmaxed probabilities:\n",
        "\n",
        "$$ p_i = \\frac{exp(x_i)}{\\sum_{j=1}^{n} exp(x_j)} $$\n",
        "\n",
        "This can be rewritten as:\n",
        "\n",
        "$$p_i = exp \\left( x_i - log\\sum_{j=1}^{n}exp(x_j) \\right)$$\n",
        "\n",
        "We can rewrite the LSE term, as well:\n",
        "\n",
        "$$ y = log\\sum_{j=1}^{n}exp(x_j) $$\n",
        "$$exp(y) = \\sum_{j=1}^{n}exp(x_j)$$\n",
        "$$exp(y) = exp(c)\\sum_{j=1}^{n}exp(x_j - c)$$\n",
        "$$y = c + log\\sum_{j=1}^{n}exp(x_j - c)$$\n",
        "\n",
        "**So a common trick is to subtract the maximum logit value from all logits before performing the softmax operation.**"
      ],
      "metadata": {
        "id": "BvJFMwaWzXGh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.special import logsumexp"
      ],
      "metadata": {
        "id": "WPMIunme4T9J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.array([1000, 1000, 1000])\n",
        "np.exp(x)"
      ],
      "metadata": {
        "id": "9RaCuUJX07jl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logsumexp(x)"
      ],
      "metadata": {
        "id": "6VfUyEPv08v5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.exp(x - logsumexp(x))"
      ],
      "metadata": {
        "id": "GMJhi7Hn0972"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XL4ndsq5PXr9"
      },
      "source": [
        "### MNIST example"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mnist = tf.keras.datasets.mnist\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train = x_train / 255.0\n",
        "# Add a channels dim\n",
        "x_train = tf.expand_dims(x_train.astype('float32'), axis=-1)\n",
        "y_train = y_train\n",
        "\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(1000).batch(32)"
      ],
      "metadata": {
        "id": "lvEnPsy0veWU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5657f724-ab31-4321-e65d-bafa2cadb3fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7CmGcH1FPXr-"
      },
      "outputs": [],
      "source": [
        "class MnistNet(tf.keras.Model):\n",
        "    def __init__(self, input_shape, loss, optimizer):\n",
        "        super(MnistNet, self).__init__()\n",
        "        self.mnist_net = tf.keras.Sequential([\n",
        "            tf.keras.layers.Input(input_shape),\n",
        "            tf.keras.layers.Flatten(name='flatten'),\n",
        "            tf.keras.layers.Dense(128, activation='relu', name='dense_1'),\n",
        "            tf.keras.layers.Dense(10, name='dense_out', activation=None)\n",
        "        ])\n",
        "        self.optimizer = optimizer\n",
        "        self.loss = loss\n",
        "\n",
        "    def summary(self):\n",
        "        self.mnist_net.summary()\n",
        "\n",
        "    def call(self, images):\n",
        "        return self.mnist_net(images)\n",
        "\n",
        "    def get_pred_loss(self, images, labels):\n",
        "        y_pred = self(images)\n",
        "        return y_pred, self.loss(labels, y_pred)\n",
        "\n",
        "    def grad_step(self, images, labels):\n",
        "        with tf.GradientTape() as tape:\n",
        "            y_pred, loss = self.get_pred_loss(images, labels)\n",
        "        gradients = tape.gradient(loss, self.trainable_variables)\n",
        "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
        "        return y_pred, loss, gradients\n",
        "\n",
        "# Create models\n",
        "loss_logits_false = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
        "loss_logits_true = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "optimizer_0 = tf.keras.optimizers.Adam()\n",
        "optimizer_1 = tf.keras.optimizers.Adam()\n",
        "\n",
        "model_logits_false = MnistNet(x_train.shape[1:], loss_logits_false, optimizer_0)\n",
        "model_logits_true = MnistNet(x_train.shape[1:], loss_logits_true, optimizer_1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, epochs, dataset, save_dir):\n",
        "\n",
        "    writer = make_writer(os.path.join('tb_logs'), save_dir)\n",
        "    train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
        "    train_loss = tf.keras.metrics.Mean()\n",
        "\n",
        "\n",
        "    for epoch in range(0, epochs + 1):\n",
        "\n",
        "        train_accuracy.reset_state()\n",
        "        train_loss.reset_state()\n",
        "\n",
        "        for images, labels in dataset:\n",
        "          # Gradient update step\n",
        "          y_pred, loss, gradients = model.grad_step(images, labels)\n",
        "          train_loss(loss)\n",
        "          train_accuracy(labels, y_pred)\n",
        "\n",
        "        # Tensorboard\n",
        "        with writer.as_default():\n",
        "            tf.summary.scalar('Train loss', loss, step=epoch)\n",
        "            tf.summary.scalar('Train Accuracy', train_accuracy.result() * 100, step=epoch)\n",
        "\n",
        "            for layer_number, layer in enumerate(model.trainable_variables):\n",
        "              tf.summary.histogram('/'.join(layer.path.split('/')[1:]), gradients[layer_number], step=epoch, buckets=1)\n",
        "\n",
        "\n",
        "        message = (f'Epoch: {epoch}, Loss: {train_loss.result()}, Accuracy: {train_accuracy.result() * 100}')\n",
        "        print(message)"
      ],
      "metadata": {
        "id": "BWG0mhsj41fM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train(model_logits_false, 5, train_ds, 'loss_bug/logits_false')\n",
        "#train(model_logits_true, 5, train_ds, 'loss_bug/logits_true')"
      ],
      "metadata": {
        "id": "27XDUsob6x2v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir tb_logs/loss_bug --port 6007"
      ],
      "metadata": {
        "id": "MisTE2E3K9Hb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Numerical instabilities\n",
        "\n",
        "- Vanishing and exploding gradients.\n",
        "- Softmax over a very large value.\n",
        "- Operations including divisions by values close to zero.\n",
        "- Big policy updates in RL.\n",
        "\n",
        "#### Exploding gradients and gradient clipping"
      ],
      "metadata": {
        "id": "WD-C35I4uRI5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "M = tf.random.normal((4, 4))\n",
        "print(f'A single matrix \\n \\n {M.numpy()}')\n",
        "for i in range(100):\n",
        "    M = tf.matmul(M, tf.random.normal((4, 4)))\n",
        "\n",
        "print(f'\\nAfter multiplying 100 matrices \\n \\n {M.numpy()}')"
      ],
      "metadata": {
        "id": "9uZdUATH1BUu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Gradient clipping\n",
        "\n",
        "- Clip a gradient by norm:\n",
        "$\\textbf{g} \\gets \\frac{\\theta}{||\\textbf{g}||}\\textbf{g} $\n",
        "    - For example: $$\\textbf{g}= [-2, 3, 6]$$ $$\\theta = 5$$ $$||\\textbf{g}|| = 7$$ $$\\textbf{g} \\gets [-2, 3, 6]\\cdot \\frac{5}{7}$$\n",
        "    \n",
        "- Clip gradient by value:\n",
        "    - If $g_i < \\theta_1$, then $g_i \\gets \\theta_1$ and $g_i > \\theta_2$, then $g_i \\gets \\theta_2$\n",
        "    - For example: $$\\textbf{g}= [-2, 3, 10]$$ $$\\theta_1 = 0, \\theta_2 = 5$$  $$ \\textbf{g} \\gets [0, 3, 5]$$\n",
        "\n",
        "    \n",
        "- Clip gradient by global norm:\n",
        "    - Rescales a list of tensors so that the total norm of the vector of all their norms does not exceed a threshold.\n",
        "    - For example: $$\\textbf{g}_1 = [-2, 3, 6]$$ $$\\textbf{g}_2= [-4, 6, 12]$$ $$\\theta = 14$$ $$||\\textbf{g}_1|| = 7$$ $$||\\textbf{g}_2|| = 14$$ $$\\textbf{g}_1 \\gets [-2, 3, 6]\\cdot \\frac{14}{\\sqrt{7^2 + 14^2}}$$ $$\\textbf{g}_2 \\gets [-4, 6, 12]\\cdot \\frac{14}{\\sqrt{7^2 + 14^2}} $$\n",
        "    "
      ],
      "metadata": {
        "id": "CViIzOgL7AOE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# generate regression dataset\n",
        "X, y = make_regression(n_samples=1000, n_features=20, noise=0.1, random_state=1)\n",
        "# split into train and test\n",
        "n_train = 500\n",
        "trainX = X[:n_train, :].astype('float32')\n",
        "trainY = y[:n_train].astype('float32').reshape(-1, 1)\n",
        "\n",
        "# Creat tf.Datasets\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((trainX, trainY)).shuffle(trainX.shape[0]).batch(32)"
      ],
      "metadata": {
        "id": "hpTPF9iYzLse"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RegressorNet(tf.keras.Model):\n",
        "    def __init__(self, input_shape, optimizer):\n",
        "        super(RegressorNet, self).__init__()\n",
        "        self.optimizer = optimizer\n",
        "        self.regressor = tf.keras.Sequential([\n",
        "            tf.keras.layers.Input((input_shape, )),\n",
        "            tf.keras.layers.Dense(25, activation='relu', kernel_initializer='he_uniform', name='dense_1'),\n",
        "            tf.keras.layers.Dense(1, activation='linear', name='out')\n",
        "        ])\n",
        "\n",
        "        self.loss = tf.keras.losses.MeanSquaredError()\n",
        "\n",
        "    def summary(self):\n",
        "        self.regressor.summary()\n",
        "\n",
        "    def call(self, X):\n",
        "        return self.regressor(X)\n",
        "\n",
        "    def get_loss(self, X, y_true):\n",
        "        y_pred = self(X)\n",
        "        l2_loss = self.loss(y_true, y_pred)\n",
        "        return l2_loss\n",
        "\n",
        "    def grad_step(self, X, y_true):\n",
        "        with tf.GradientTape() as tape:\n",
        "            loss = self.get_loss(X, y_true)\n",
        "        gradients = tape.gradient(loss, self.trainable_variables)\n",
        "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
        "        return loss, gradients\n",
        "\n",
        "    def grad_step_clipped(self, X, y_true):\n",
        "        with tf.GradientTape() as tape:\n",
        "            loss = self.get_loss(X, y_true)\n",
        "        gradients = tape.gradient(loss, self.trainable_variables)\n",
        "        gradients, _ = tf.clip_by_global_norm(gradients, 1.0)\n",
        "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
        "        return loss, gradients\n",
        "\n",
        "# Specify an optimizer and instances of the model\n",
        "optimizer_not_clipped = tf.keras.optimizers.SGD(0.01, 0.9)\n",
        "optimizer_clipped = tf.keras.optimizers.SGD(0.01, 0.9)\n",
        "\n",
        "net_not_clipped = RegressorNet(input_shape=trainX.shape[1], optimizer=optimizer_not_clipped)\n",
        "net_clipped = RegressorNet(input_shape=trainX.shape[1], optimizer=optimizer_clipped)\n",
        "\n",
        "# Show summary\n",
        "net_clipped.summary()"
      ],
      "metadata": {
        "id": "sb2XH4QR1EW3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, epochs, train_dataset, save_dir, clip=False):\n",
        "\n",
        "    writer = make_writer(os.path.join('tb_logs'), save_dir)\n",
        "\n",
        "    for epoch in range(0, epochs + 1):\n",
        "\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "            print('Epoch {} is running...'.format(epoch))\n",
        "\n",
        "        for X, y in train_dataset:\n",
        "            # Gradient update step\n",
        "            if clip:\n",
        "              loss, gradients = model.grad_step_clipped(X, y)\n",
        "            else:\n",
        "              loss, gradients = model.grad_step(X, y)\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "            print(f'Train loss: {loss}')\n",
        "\n",
        "        # Tensorboard\n",
        "        with writer.as_default():\n",
        "            tf.summary.scalar('Train loss', loss, step=epoch)\n",
        "\n",
        "            for layer_number, layer in enumerate(model.trainable_variables):\n",
        "                tf.summary.histogram('/'.join(layer.path.split('/')[1:]), gradients[layer_number], step=epoch, buckets=1)\n"
      ],
      "metadata": {
        "id": "3nwn9xfC9IQt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train(net_not_clipped, 100, train_dataset, 'exploding_grads/no_clipping', clip=False)\n",
        "#train(net_clipped, 100, train_dataset, 'exploding_grads/clipped', clip=True)"
      ],
      "metadata": {
        "id": "Wx96uFt0VD1b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir tb_logs/exploding_grads --port 6008"
      ],
      "metadata": {
        "id": "EDs4Gmek2GQu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## OOM errors\n",
        "\n",
        "### Common issues and causes\n",
        "\n",
        "- Too big a tensor:\n",
        "    - Too large a batch size for your model\n",
        "    - Too many fully connected layers\n",
        "- Too much data:\n",
        "    - Loading a too big dataset into memory instead of using, e.g. tf.data queue loading\n",
        "    - Allocating to large a buffer for dataset creation\n",
        "- Duplicating operations:\n",
        "    - Memory leak due to creating multiple models at the same time\n",
        "    - Repeatedly creating an operation (e.g. in a function that gets called many times)\n",
        "- Other processes:\n",
        "    - Other processes taking GPU memory"
      ],
      "metadata": {
        "id": "mUvDalNUdpHO"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}